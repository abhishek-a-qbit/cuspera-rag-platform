\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, shadows, calc}
\usepackage{tcolorbox}
\tcbuselibrary{skins,breakable}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}

% Color scheme
\definecolor{primary}{RGB}{41, 128, 185}
\definecolor{secondary}{RGB}{142, 68, 173}
\definecolor{accent}{RGB}{230, 126, 34}
\definecolor{success}{RGB}{39, 174, 96}
\definecolor{warning}{RGB}{241, 196, 15}
\definecolor{lightgray}{RGB}{236, 240, 241}

% Custom boxes
\newtcolorbox{keypoint}[1]{
  colback=primary!5,
  colframe=primary,
  fonttitle=\bfseries,
  title=#1,
  rounded corners,
  shadow={2mm}{-1mm}{0mm}{black!20}
}

\newtcolorbox{insight}[1]{
  colback=accent!5,
  colframe=accent,
  fonttitle=\bfseries,
  title=#1,
  rounded corners,
  shadow={2mm}{-1mm}{0mm}{black!20}
}

\newtcolorbox{methodology}[1]{
  colback=secondary!5,
  colframe=secondary,
  fonttitle=\bfseries,
  title=#1,
  rounded corners,
  shadow={2mm}{-1mm}{0mm}{black!20}
}

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small\textcolor{primary}{RAG-Based FAQ Generation \& Evaluation}}
\fancyhead[R]{\small\textcolor{primary}{\thepage}}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\headrule}{\hbox to\headwidth{\color{primary}\leaders\hrule height \headrulewidth\hfill}}

\hypersetup{
    colorlinks=true,
    linkcolor=primary,
    citecolor=secondary,
    urlcolor=accent
}

\title{\Huge\textbf{\textcolor{primary}{Comprehensive Framework for}} \\[0.3cm]
\Huge\textbf{\textcolor{secondary}{RAG-Based FAQ Generation}} \\[0.3cm]
\Huge\textbf{\textcolor{accent}{and Evaluation}}}
\author{\Large Advanced Metrics \& Quality Assurance Pipeline}
\date{\today}

\begin{document}

\maketitle
\thispagestyle{empty}

\begin{abstract}
\noindent This report presents a comprehensive framework for generating and evaluating Frequently Asked Questions (FAQs) using Retrieval-Augmented Generation (RAG). We introduce a multi-dimensional evaluation methodology encompassing four key metrics: \textbf{Coverage}, \textbf{Specificity}, \textbf{Insightfulness}, and \textbf{Groundedness}. Our approach combines both statistical/mathematical graders and LLM-based evaluators to ensure robust quality assessment. Furthermore, we propose systematic strategies to \emph{enforce} these quality dimensions during the generation phase, moving beyond mere measurement to proactive quality assurance.
\end{abstract}

\newpage
\tableofcontents
\newpage

\section{Introduction}

\subsection{Motivation}

The proliferation of digital content has created an urgent need for effective knowledge management systems. FAQs serve as a critical interface between users and information repositories, yet manually creating comprehensive, high-quality FAQs is labor-intensive and often incomplete.

\begin{keypoint}{The Challenge}
Traditional FAQ generation faces three critical challenges:
\begin{enumerate}[leftmargin=*]
    \item \textbf{Scalability}: Manual curation doesn't scale with growing content
    \item \textbf{Coverage}: Ensuring all relevant topics are addressed
    \item \textbf{Quality}: Maintaining consistency, accuracy, and insightfulness
\end{enumerate}
\end{keypoint}

\subsection{Our Approach}

We propose a \textbf{keyword-driven RAG pipeline} that:
\begin{itemize}[leftmargin=*]
    \item Leverages user engagement data (high-traffic keywords) to ensure relevant coverage
    \item Generates hypothetical questions and answers from chunked source material
    \item Employs hybrid evaluation (statistical + LLM graders) for quality assessment
    \item Enforces quality constraints during generation, not just post-hoc evaluation
\end{itemize}

\begin{center}
\begin{tikzpicture}[
  node distance=1.5cm,
  box/.style={rectangle, rounded corners, minimum width=2.8cm, minimum height=1cm, text centered, draw=black, fill=primary!20, font=\small\bfseries, drop shadow},
  arrow/.style={-{Stealth[length=3mm]}, thick, color=primary}
]

\node (keywords) [box, fill=success!30] {Keywords Dataset};
\node (scraping) [box, right=of keywords, fill=accent!30] {Web Scraping};
\node (chunks) [box, right=of scraping, fill=warning!30] {Data Chunks};
\node (generation) [box, below=of chunks, fill=secondary!30] {Q\&A Generation};
\node (evaluation) [box, left=of generation, fill=primary!30] {Hybrid Evaluation};
\node (excel) [box, left=of evaluation, fill=success!30] {Analysis \& Iteration};

\draw [arrow] (keywords) -- (scraping);
\draw [arrow] (scraping) -- (chunks);
\draw [arrow] (chunks) -- (generation);
\draw [arrow] (generation) -- (evaluation);
\draw [arrow] (evaluation) -- (excel);
\draw [arrow, dashed, color=accent] (excel) to[bend right=45] node[above, sloped, font=\tiny] {feedback loop} (generation);

\end{tikzpicture}
\end{center}

\section{The Four-Dimensional Evaluation Framework}

Our evaluation framework assesses both questions and answers across four critical dimensions. Each dimension captures a distinct aspect of quality.

\subsection{Coverage: Breadth of Information}

\begin{methodology}{Coverage Definition}
Coverage measures whether the generated FAQs adequately span the topic space present in the source data. For questions, it evaluates topical diversity; for answers, it assesses completeness.
\end{methodology}

\subsubsection{Mathematical Formulation}

\textbf{For Questions (Topic Coverage):}

Let $\mathcal{D} = \{d_1, d_2, \ldots, d_n\}$ be the set of data chunks and $\mathcal{Q} = \{q_1, q_2, \ldots, q_m\}$ be the generated questions. We define coverage as:

\begin{equation}
\text{Coverage}_Q = \frac{1}{|\mathcal{T}|} \sum_{t \in \mathcal{T}} \max_{q \in \mathcal{Q}} \text{sim}(t, q)
\end{equation}

where $\mathcal{T}$ is the set of topics extracted from $\mathcal{D}$, and $\text{sim}(\cdot, \cdot)$ is a semantic similarity function (e.g., cosine similarity of embeddings).

\textbf{Diversity Metric:}

\begin{equation}
\text{Diversity}_Q = \frac{2}{m(m-1)} \sum_{i=1}^{m-1} \sum_{j=i+1}^{m} (1 - \text{sim}(q_i, q_j))
\end{equation}

Higher diversity indicates better coverage of distinct topics.

\textbf{For Answers (Completeness):}

Given question $q$ and answer $a$ with relevant context chunks $\mathcal{C}_q = \{c_1, \ldots, c_k\}$:

\begin{equation}
\text{Coverage}_A(a, q) = \frac{\sum_{c \in \mathcal{C}_q} \mathbb{1}[\text{addressed}(a, c)]}{|\mathcal{C}_q|}
\end{equation}

where $\mathbb{1}[\text{addressed}(a, c)]$ indicates whether information from chunk $c$ is present in answer $a$.

\subsection{Specificity: Precision and Detail}

\begin{methodology}{Specificity Definition}
Specificity quantifies how precise, detailed, and concrete the content is, as opposed to vague or generic statements.
\end{methodology}

\subsubsection{Mathematical Formulation}

\textbf{Named Entity Density:}

\begin{equation}
\text{NED}(text) = \frac{|\text{entities}(text)|}{|\text{tokens}(text)|}
\end{equation}

\textbf{Lexical Density:}

\begin{equation}
\text{LD}(text) = \frac{|\text{content\_words}(text)|}{|\text{tokens}(text)|}
\end{equation}

\textbf{Hedge Word Penalty:}

\begin{equation}
\text{HWP}(text) = 1 - \frac{|\text{hedge\_words}(text)|}{|\text{tokens}(text)|}
\end{equation}

where hedge words include: \emph{might, possibly, usually, perhaps, generally}, etc.

\textbf{Composite Specificity Score:}

\begin{equation}
\text{Specificity}(text) = w_1 \cdot \text{NED}(text) + w_2 \cdot \text{LD}(text) + w_3 \cdot \text{HWP}(text)
\end{equation}

with weights $w_1, w_2, w_3$ summing to 1.

\subsection{Insightfulness: Depth and Value}

\begin{methodology}{Insightfulness Definition}
Insightfulness measures whether the content provides valuable, non-obvious information that enhances understanding beyond surface-level facts.
\end{methodology}

\subsubsection{Mathematical Formulation}

\textbf{Information Gain:}

\begin{equation}
\text{IG}(q, a) = H(q) - H(a|q)
\end{equation}

where $H(\cdot)$ is entropy. Higher information gain indicates the answer substantially reduces uncertainty.

\textbf{Semantic Distance (Novelty):}

\begin{equation}
\text{Novelty}(q, a) = 1 - \text{sim}(\text{embed}(q), \text{embed}(a))
\end{equation}

Greater semantic distance suggests the answer provides information beyond what's obvious from the question.

\textbf{Complexity Indicators:}

\begin{equation}
\text{Complexity}(text) = \alpha \cdot \text{parse\_depth}(text) + \beta \cdot \text{causal\_markers}(text)
\end{equation}

where causal markers include: \emph{because, therefore, thus, hence, consequently}.

\subsection{Groundedness: Factual Fidelity}

\begin{methodology}{Groundedness Definition}
Groundedness (equivalent to RAGAS Faithfulness) ensures that every claim in the generated content is supported by the source material, with no hallucinations.
\end{methodology}

\subsubsection{Mathematical Formulation}

\textbf{Claim-Level Verification:}

Let $a = \{s_1, s_2, \ldots, s_p\}$ be the set of claims/statements in answer $a$, and $\mathcal{C}$ be the context chunks.

\begin{equation}
\text{Groundedness}(a, \mathcal{C}) = \frac{1}{p} \sum_{i=1}^{p} \max_{c \in \mathcal{C}} \text{entailment}(s_i, c)
\end{equation}

where $\text{entailment}(s, c) \in [0, 1]$ is computed via NLI models or embedding similarity.

\textbf{N-gram Overlap:}

\begin{equation}
\text{NGO}_k(a, \mathcal{C}) = \frac{|\text{ngrams}_k(a) \cap \text{ngrams}_k(\mathcal{C})|}{|\text{ngrams}_k(a)|}
\end{equation}

\textbf{ROUGE-L Score:}

\begin{equation}
\text{ROUGE-L}(a, c) = \frac{(1 + \beta^2) \cdot R_{\text{lcs}} \cdot P_{\text{lcs}}}{\beta^2 \cdot R_{\text{lcs}} + P_{\text{lcs}}}
\end{equation}

where $R_{\text{lcs}}$ is recall and $P_{\text{lcs}}$ is precision of longest common subsequence.

\section{Hybrid Evaluation: Statistical + LLM Graders}

\begin{insight}{Why Hybrid?}
Statistical methods provide objective, reproducible metrics but may miss semantic nuances. LLM graders capture human-like judgment but can be inconsistent. Combining both yields robust, comprehensive evaluation.
\end{insight}

\subsection{Statistical/Mathematical Graders}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Metric} & \textbf{Statistical Approach} & \textbf{Formulation} \\
\midrule
Coverage & Topic Modeling (LDA/BERTopic) & $P(topic|Q) \sim \text{Dirichlet}(\alpha)$ \\
 & TF-IDF Similarity & $\text{TF-IDF}(t,d) = tf(t,d) \cdot \log\frac{N}{df(t)}$ \\
\midrule
Specificity & Named Entity Density & $\text{NED} = \frac{|\text{NE}|}{|tokens|}$ \\
 & Perplexity (Domain LM) & $\text{PPL} = \exp\left(-\frac{1}{N}\sum \log P(w_i)\right)$ \\
\midrule
Insightfulness & Inverse Document Freq. & $\text{IDF}(t) = \log\frac{N}{n_t}$ \\
 & Surprisal Score & $-\log P(text|model)$ \\
\midrule
Groundedness & Cosine Similarity & $\cos(\mathbf{a}, \mathbf{c}) = \frac{\mathbf{a} \cdot \mathbf{c}}{\|\mathbf{a}\|\|\mathbf{c}\|}$ \\
 & BM25 Score & $\sum_{t} \text{IDF}(t) \cdot \frac{f(t) \cdot (k+1)}{f(t) + k \cdot (1-b+b\cdot\frac{|d|}{avgdl})}$ \\
\bottomrule
\end{tabular}
\caption{Statistical Graders by Metric}
\end{table}

\subsection{LLM-Based Graders}

\textbf{General LLM Grader Architecture:}

\begin{algorithm}
\caption{LLM Grader with Few-Shot Learning}
\begin{algorithmic}[1]
\State \textbf{Input:} $text$, $metric$, $few\_shot\_examples$
\State $prompt \gets$ Construct prompt with:
\State \quad - Metric definition and rubric
\State \quad - Few-shot examples with scores and reasoning
\State \quad - The text to evaluate
\State $response \gets$ LLM($prompt$, temperature=0.1)
\State Parse $score$ and $reasoning$ from $response$
\State \textbf{Return:} $(score, reasoning)$
\end{algorithmic}
\end{algorithm}

\textbf{Example Prompt Template:}

\begin{tcolorbox}[colback=lightgray, colframe=primary, title=LLM Grader Prompt (Specificity)]
\small
\textbf{Task:} Rate the specificity of the following question on a scale of 1-5.

\textbf{Rubric:}
\begin{itemize}
    \item 1: Extremely vague or generic
    \item 3: Moderately specific with some concrete details
    \item 5: Highly specific with concrete parameters, scenarios, or entities
\end{itemize}

\textbf{Examples:}
\begin{itemize}
    \item \textit{Question:} "How does authentication work?" \\ \textit{Score:} 2 \\ \textit{Reason:} Too broad, no specific mechanism mentioned
    \item \textit{Question:} "How do I implement OAuth 2.0 token refresh when the access token expires after 3600 seconds?" \\ \textit{Score:} 5 \\ \textit{Reason:} Highly specific with concrete technology (OAuth 2.0), action (token refresh), and parameter (3600 seconds)
\end{itemize}

\textbf{Evaluate this question:} [INSERT QUESTION]

\textbf{Provide your score (1-5) and reasoning:}
\end{tcolorbox}

\subsection{Score Combination Strategy}

Given statistical score $s_{stat} \in [0, 1]$ and LLM score $s_{LLM} \in [1, 5]$:

\begin{equation}
s_{LLM}^{\text{norm}} = \frac{s_{LLM} - 1}{4}
\end{equation}

\begin{equation}
s_{final} = \lambda \cdot s_{stat} + (1-\lambda) \cdot s_{LLM}^{\text{norm}}
\end{equation}

where $\lambda \in [0, 1]$ is a weight parameter (typically $\lambda = 0.5$ for equal weighting).

\section{Ensuring Quality During Generation}

\begin{keypoint}{Paradigm Shift}
Rather than only \emph{measuring} quality post-generation, we \emph{enforce} quality constraints during the generation phase through careful prompt engineering, structural constraints, and verification loops.
\end{keypoint}

\subsection{Coverage Assurance}

\textbf{Strategy:} Keyword-driven chunk-based generation

\begin{algorithm}
\caption{Coverage-Assured Question Generation}
\begin{algorithmic}[1]
\State \textbf{Input:} $keywords$, $chunks$
\State $questions \gets \{\}$
\For{each $chunk_i \in chunks$}
    \State $relevant\_keywords \gets$ Extract keywords present in $chunk_i$
    \For{each $kw \in relevant\_keywords$}
        \State $q \gets$ GenerateQuestion($chunk_i$, $kw$, constraints)
        \State $questions \gets questions \cup \{q\}$
    \EndFor
\EndFor
\State \textbf{Verify:} $\forall chunk_i, \exists q \in questions$ generated from $chunk_i$
\State \textbf{Return:} $questions$
\end{algorithmic}
\end{algorithm}

\textbf{Mathematical Guarantee:}

If we generate at least one question per chunk, and chunks cover all topics $\mathcal{T}$:

\begin{equation}
\text{Coverage}_Q \geq \min_{t \in \mathcal{T}} \max_{q \in \mathcal{Q}} \text{sim}(t, q) > \tau
\end{equation}

for some threshold $\tau > 0$.

\subsection{Specificity Enforcement}

\subsubsection{Question Generation Constraints}

\begin{methodology}{Specificity Constraints}
\textbf{Prohibited Patterns:}
\begin{itemize}
    \item Generic question starters: "What is...", "Tell me about..."
    \item Overly broad scope without qualifiers
\end{itemize}

\textbf{Required Elements:}
\begin{itemize}
    \item At least one named entity or specific parameter
    \item Concrete scenario or use case
    \item Minimum length threshold (e.g., 10 tokens)
\end{itemize}

\textbf{Encouraged Patterns:}
\begin{itemize}
    \item "How do I [action] when [condition]?"
    \item "What is the difference between [X] and [Y] in [context]?"
    \item "Why does [phenomenon] occur in [scenario]?"
\end{itemize}
\end{methodology}

\subsubsection{Answer Generation Constraints}

\begin{itemize}
    \item \textbf{Format enforcement:} "Answer must include: [Specific fact/number], [Step-by-step if applicable], [Concrete example]"
    \item \textbf{Hedge word limit:} Maximum 2\% of tokens can be hedge words
    \item \textbf{Entity injection:} Extract entities from context; mandate inclusion in answer
\end{itemize}

\subsection{Insightfulness Enforcement}

\begin{center}
\begin{tikzpicture}[
  node distance=1.2cm and 0.8cm,
  box/.style={rectangle, rounded corners, minimum width=2.5cm, minimum height=0.8cm, text width=2.3cm, text centered, draw=black, font=\tiny\bfseries, drop shadow},
  process/.style={box, fill=secondary!30},
  check/.style={diamond, aspect=2, minimum width=2cm, text width=1.8cm, text centered, draw=black, fill=warning!30, font=\tiny\bfseries, drop shadow}
]

\node (start) [box, fill=success!30] {Source Chunk};
\node (extract) [process, below=of start] {Extract: Edge Cases, Trade-offs};
\node (category) [process, below=of extract] {Categorize: Why/How/Optimize};
\node (generate) [process, below=of category] {Generate Question};
\node (check1) [check, below=of generate] {Depth Check: Causal/Comparative?};
\node (regenerate) [process, right=of check1] {Regenerate with Depth Prompt};
\node (answer) [process, below=of check1] {Generate Answer};
\node (check2) [check, below=of answer] {Insight Check: Context/Tips?};
\node (enhance) [process, right=of check2] {Enhance with Why/Implications};
\node (final) [box, fill=success!30, below=of check2] {Insightful Q\&A};

\draw [-{Stealth[length=2mm]}, thick] (start) -- (extract);
\draw [-{Stealth[length=2mm]}, thick] (extract) -- (category);
\draw [-{Stealth[length=2mm]}, thick] (category) -- (generate);
\draw [-{Stealth[length=2mm]}, thick] (generate) -- (check1);
\draw [-{Stealth[length=2mm]}, thick] (check1) -- node[right, font=\tiny] {No} (regenerate);
\draw [-{Stealth[length=2mm]}, thick] (regenerate) |- (generate);
\draw [-{Stealth[length=2mm]}, thick] (check1) -- node[right, font=\tiny] {Yes} (answer);
\draw [-{Stealth[length=2mm]}, thick] (answer) -- (check2);
\draw [-{Stealth[length=2mm]}, thick] (check2) -- node[right, font=\tiny] {No} (enhance);
\draw [-{Stealth[length=2mm]}, thick] (enhance) |- (answer);
\draw [-{Stealth[length=2mm]}, thick] (check2) -- node[right, font=\tiny] {Yes} (final);

\end{tikzpicture}
\end{center}

\textbf{Question Categories for Insight:}
\begin{itemize}
    \item \textbf{Troubleshooting:} "What causes [problem] and how to fix it?"
    \item \textbf{Optimization:} "How to improve/optimize [process]?"
    \item \textbf{Edge Cases:} "What happens when [unusual scenario]?"
    \item \textbf{Comparisons:} "What are the trade-offs between [X] and [Y]?"
    \item \textbf{Best Practices:} "What's the recommended approach for [task] and why?"
\end{itemize}

\subsection{Groundedness Enforcement}

\begin{methodology}{Strict RAG Protocol}
\textbf{Generation Constraints:}
\begin{enumerate}
    \item \textbf{Explicit instruction:} "Answer ONLY using information from the provided context"
    \item \textbf{Low temperature:} Use $T = 0.1$-$0.3$ for deterministic, grounded responses
    \item \textbf{Citation forcing:} Require inline attribution for each claim
    \item \textbf{Pre-generation check:} Verify sufficient information exists in context
\end{enumerate}

\textbf{Post-Generation Verification:}
\begin{enumerate}
    \item Extract claims from generated answer: $\{s_1, s_2, \ldots, s_p\}$
    \item For each claim $s_i$, compute $\max_{c \in \mathcal{C}} \text{entailment}(s_i, c)$
    \item If $\exists s_i: \text{entailment}(s_i, c) < \tau$ for all $c$, regenerate or remove claim
\end{enumerate}
\end{methodology}

\begin{algorithm}
\caption{Groundedness-Assured Answer Generation}
\begin{algorithmic}[1]
\State \textbf{Input:} $question$, $context\_chunks$
\State $answer \gets$ Generate($question$, $context\_chunks$, $T=0.2$, strict\_mode=True)
\State $claims \gets$ ExtractClaims($answer$)
\State $grounded\_claims \gets \{\}$
\For{each $claim \in claims$}
    \State $support \gets \max_{c \in context\_chunks} \text{Entailment}(claim, c)$
    \If{$support > \tau$}  \Comment{Threshold e.g. 0.7}
        \State $grounded\_claims \gets grounded\_claims \cup \{claim\}$
    \EndIf
\EndFor
\If{$|\grounded\_claims| < 0.9 \cdot |claims|$}  \Comment{At least 90\% grounded}
    \State $answer \gets$ Regenerate with stricter constraints
\EndIf
\State \textbf{Return:} $answer$
\end{algorithmic}
\end{algorithm}

\section{Complete Evaluation Pipeline}

\subsection{Pipeline Architecture}

\begin{center}
\begin{tikzpicture}[
  node distance=1cm and 1.5cm,
  box/.style={rectangle, rounded corners, minimum width=2.2cm, minimum height=0.9cm, text width=2cm, text centered, draw=black, font=\scriptsize\bfseries, drop shadow},
  data/.style={box, fill=success!20},
  process/.style={box, fill=primary!20},
  eval/.style={box, fill=accent!20},
  output/.style={box, fill=secondary!20}
]

\node (keywords) [data] {Keywords};
\node (chunks) [data, right=of keywords] {Data Chunks};
\node (qgen) [process, below=of $(keywords)!0.5!(chunks)$] {Q Generation};
\node (agen) [process, below=of qgen] {A Generation};
\node (stat) [eval, below left=of agen] {Statistical Graders};
\node (llm) [eval, below right=of agen] {LLM Graders};
\node (combine) [process, below=of agen] {Score Fusion};
\node (excel) [output, below=of combine] {Excel Analysis};
\node (iterate) [process, right=of excel] {Iteration \& Refinement};

\draw [-{Stealth[length=2mm]}, thick] (keywords) -- (qgen);
\draw [-{Stealth[length=2mm]}, thick] (chunks) -- (qgen);
\draw [-{Stealth[length=2mm]}, thick] (qgen) -- (agen);
\draw [-{Stealth[length=2mm]}, thick] (agen) -- (stat);
\draw [-{Stealth[length=2mm]}, thick] (agen) -- (llm);
\draw [-{Stealth[length=2mm]}, thick] (stat) -- (combine);
\draw [-{Stealth[length=2mm]}, thick] (llm) -- (combine);
\draw [-{Stealth[length=2mm]}, thick] (combine) -- (excel);
\draw [-{Stealth[length=2mm]}, thick] (excel) -- (iterate);
\draw [-{Stealth[length=2mm]}, thick, dashed, color=accent] (iterate) to[bend left=60] (qgen);

\end{tikzpicture}
\end{center}

\subsection{Excel Output Schema}

\begin{table}[h]
\centering
\tiny
\begin{tabular}{@{}llllllll@{}}
\toprule
\textbf{Question} & \textbf{Answer} & \textbf{Cov\_Math} & \textbf{Cov\_LLM} & \textbf{Spec\_Math} & \textbf{Spec\_LLM} & \textbf{...} & \textbf{Final\_Score} \\
\midrule
Q1 & A1 & 0.82 & 4.0 & 0.75 & 3.5 & ... & 0.78 \\
Q2 & A2 & 0.91 & 4.5 & 0.88 & 4.5 & ... & 0.90 \\
... & ... & ... & ... & ... & ... & ... & ... \\
\bottomrule
\end{tabular}
\caption{Excel Analysis Schema}
\end{table}

Each row represents one Q\&A pair, with columns for:
\begin{itemize}
    \item Raw question and answer text
    \item Mathematical scores (normalized 0-1) for each metric
    \item LLM scores (1-5) for each metric
    \item Intermediate combined scores
    \item Final aggregated quality score
    \item Metadata (chunk ID, keyword, generation timestamp)
\end{itemize}

\section{Advanced Topics}

\subsection{Handling Metric Trade-offs}

Different metrics can be in tension. For example:

\begin{insight}{Trade-off Example}
\textbf{Specificity vs. Coverage:} Highly specific questions may cover narrow topics deeply but sacrifice breadth. Conversely, broad coverage might result in less specific questions. Balance is achieved through strategic keyword selection and chunk granularity.
\end{insight}

\textbf{Multi-Objective Optimization:}

We can formulate FAQ generation as a multi-objective optimization problem:

\begin{equation}
\max_{Q,A} \left\{ \alpha_1 \cdot \text{Coverage}(Q,A) + \alpha_2 \cdot \text{Specificity}(Q,A) + \alpha_3 \cdot \text{Insightfulness}(Q,A) + \alpha_4 \cdot \text{Groundedness}(Q,A) \right\}
\end{equation}

subject to constraints:
\begin{align}
\text{Groundedness}(Q,A) &\geq \tau_g \quad \text{(hard constraint)} \\
\sum_{i=1}^{4} \alpha_i &= 1, \quad \alpha_i \geq 0
\end{align}

where $\tau_g$ is a minimum groundedness threshold (e.g., 0.8), and weights $\alpha_i$ are tunable based on use case.

\subsection{Pareto Frontier Analysis}

For conflicting objectives, we can visualize the Pareto frontier:

\begin{center}
\begin{tikzpicture}[scale=1.2]
\draw[->] (0,0) -- (5,0) node[right] {Specificity};
\draw[->] (0,0) -- (0,4) node[above] {Coverage};

% Pareto frontier curve
\draw[thick, color=accent, domain=0.5:4.5, smooth, variable=\x] 
  plot ({\x}, {3.5 - 0.15*(\x-2.5)^2});

% Dominated region
\fill[primary!10] (0.5,0) -- plot[domain=0.5:4.5, smooth] ({\x}, {3.5 - 0.15*(\x-2.5)^2}) -- (4.5,0) -- cycle;

% Sample points
\filldraw[color=success, fill=success!50] (1,2.8) circle (2pt) node[above left, font=\tiny] {A};
\filldraw[color=success, fill=success!50] (2.5,3.4) circle (2pt) node[above, font=\tiny] {B (Optimal)};
\filldraw[color=success, fill=success!50] (4,2.9) circle (2pt) node[above right, font=\tiny] {C};
\filldraw[color=warning, fill=warning!50] (2,2) circle (2pt) node[below, font=\tiny] {D (Dominated)};

\node[font=\tiny] at (2.5,0.5) {Feasible Region};
\draw[dashed, color=accent] (0.5,3.4) -- (4.5,3.4) node[right, font=\tiny] {Pareto Frontier};

\end{tikzpicture}
\end{center}

Points A, B, C lie on the Pareto frontier (optimal trade-offs), while D is dominated (can improve both metrics).

\subsection{Context Window Management}

For very large corpora, chunk selection becomes critical:

\textbf{Chunk Relevance Scoring:}

\begin{equation}
\text{Relevance}(chunk, keyword) = \lambda_{sem} \cdot \text{sem\_sim}(chunk, kw) + \lambda_{freq} \cdot \text{freq}(kw, chunk)
\end{equation}

Select top-$k$ chunks per keyword to balance coverage and computational efficiency.

\subsection{Temporal Dynamics}

For evolving content, implement version tracking:

\begin{equation}
\text{Freshness}(Q_t, A_t) = e^{-\beta(t_{current} - t_{generated})}
\end{equation}

where $\beta$ controls decay rate. Prioritize regeneration when freshness drops below threshold.

\section{Few-Shot Learning Examples}

\subsection{Specificity Examples}

\begin{tcolorbox}[colback=lightgray, colframe=secondary, breakable]
\textbf{Question Specificity Examples:}

\begin{enumerate}
\item \textbf{Question:} "How does the system work?" \\
\textbf{Score:} 1/5 \\
\textbf{Reason:} Extremely vague, no specific component or process mentioned

\item \textbf{Question:} "How do I use the API?" \\
\textbf{Score:} 2/5 \\
\textbf{Reason:} Slightly specific (API mentioned) but still too broad

\item \textbf{Question:} "How do I authenticate API requests using OAuth 2.0?" \\
\textbf{Score:} 4/5 \\
\textbf{Reason:} Specific technology (OAuth 2.0) and action (authenticate) mentioned

\item \textbf{Question:} "How do I implement OAuth 2.0 authorization code flow with PKCE for mobile apps to handle token refresh when the access token expires after 3600 seconds?" \\
\textbf{Score:} 5/5 \\
\textbf{Reason:} Highly specific with concrete flow type (authorization code with PKCE), use case (mobile apps), parameter (3600 seconds), and action (token refresh)
\end{enumerate}

\textbf{Answer Specificity Examples:}

\begin{enumerate}
\item \textbf{Answer:} "You can reset your password in the settings." \\
\textbf{Score:} 2/5 \\
\textbf{Reason:} Vague location, no concrete steps

\item \textbf{Answer:} "To reset your password: 1) Go to Settings, 2) Click Security, 3) Click Change Password, 4) Enter new password (min 8 characters), 5) Save." \\
\textbf{Score:} 4/5 \\
\textbf{Reason:} Step-by-step with specific requirement (8 characters)

\item \textbf{Answer:} "To reset your password: 1) Click the gear icon (⚙️) in the top-right corner, 2) Select 'Security \& Privacy' from the left sidebar, 3) Under 'Password Management', click 'Change Password', 4) Enter your current password for verification, 5) Enter new password meeting requirements (minimum 12 characters, one uppercase, one lowercase, one number, one special character from !@\#\$\%\^{}\&*), 6) Confirm new password, 7) Click 'Update Password'. Your session will remain active, but you'll need the new password for future logins. Changes take effect immediately across all devices." \\
\textbf{Score:} 5/5 \\
\textbf{Reason:} Extremely detailed with specific UI elements, exact requirements, and practical implications
\end{enumerate}
\end{tcolorbox}

\subsection{Insightfulness Examples}

\begin{tcolorbox}[colback=lightgray, colframe=accent, breakable]
\textbf{Question Insightfulness Examples:}

\begin{enumerate}
\item \textbf{Question:} "What is machine learning?" \\
\textbf{Score:} 1/5 \\
\textbf{Reason:} Basic definitional question, no depth

\item \textbf{Question:} "How does machine learning work?" \\
\textbf{Score:} 2/5 \\
\textbf{Reason:} Still surface-level, asks for general explanation

\item \textbf{Question:} "What's the difference between supervised and unsupervised learning?" \\
\textbf{Score:} 3/5 \\
\textbf{Reason:} Comparative question shows some depth

\item \textbf{Question:} "Why do gradient boosting models often outperform random forests on tabular data?" \\
\textbf{Score:} 4/5 \\
\textbf{Reason:} Asks 'why', explores performance trade-offs, specific to use case

\item \textbf{Question:} "Why do gradient boosting models often outperform random forests on tabular data with complex feature interactions, and when should you choose random forests instead to avoid overfitting on small datasets?" \\
\textbf{Score:} 5/5 \\
\textbf{Reason:} Explores causality, trade-offs, edge cases, and practical decision-making criteria
\end{enumerate}

\textbf{Answer Insightfulness Examples:}

\begin{enumerate}
\item \textbf{Answer:} "We support OAuth 2.0 authentication." \\
\textbf{Score:} 1/5 \\
\textbf{Reason:} Simple fact, no context or depth

\item \textbf{Answer:} "We support OAuth 2.0 authentication, which allows secure login without sharing passwords." \\
\textbf{Score:} 2/5 \\
\textbf{Reason:} Basic benefit mentioned but minimal depth

\item \textbf{Answer:} "We support OAuth 2.0 authentication using the authorization code flow. This is more secure than the implicit flow because tokens aren't exposed in the browser URL, reducing the risk of token theft." \\
\textbf{Score:} 4/5 \\
\textbf{Reason:} Explains why (security), compares approaches, provides reasoning

\item \textbf{Answer:} "We support OAuth 2.0 authentication using the authorization code flow, which is more secure than the implicit flow because tokens aren't exposed in the browser URL. This is particularly important for single-page applications where the URL history could leak sensitive tokens. Implementation note: Our refresh tokens expire after 30 days of inactivity, so implement token refresh logic to maintain long-running sessions. Common pitfall: Don't store tokens in localStorage—use httpOnly cookies to prevent XSS attacks. For mobile apps, consider using PKCE (Proof Key for Code Exchange) extension to protect against authorization code interception attacks." \\
\textbf{Score:} 5/5 \\
\textbf{Reason:} Comprehensive explanation with why, practical implications, implementation guidance, common pitfalls, and security best practices
\end{enumerate}
\end{tcolorbox}

\subsection{Groundedness Examples}

\begin{tcolorbox}[colback=lightgray, colframe=primary, breakable]
\textbf{Context:} "Our API supports up to 10,000 requests per hour per API key. Rate limiting uses a sliding window algorithm."

\textbf{Answer Examples:}

\begin{enumerate}
\item \textbf{Answer:} "Our API supports 10,000 requests per hour, which is competitive with industry standards and should handle most production workloads efficiently." \\
\textbf{Score:} 2/5 \\
\textbf{Reason:} First part grounded, but "industry standards" and "most production workloads" are NOT in the context—hallucinated information

\item \textbf{Answer:} "The API rate limit is 10,000 requests per hour per API key, implemented using a sliding window algorithm." \\
\textbf{Score:} 5/5 \\
\textbf{Reason:} Perfectly grounded—every claim directly supported by context

\item \textbf{Answer:} "We use a sliding window rate limiting algorithm with a limit of 10,000 requests per hour per key." \\
\textbf{Score:} 5/5 \\
\textbf{Reason:} Paraphrased but fully grounded in provided context
\end{enumerate}
\end{tcolorbox}

\section{Implementation Pseudocode}

\subsection{Complete Generation \& Evaluation Pipeline}

\begin{algorithm}[H]
\caption{Complete FAQ Generation \& Evaluation Pipeline}
\begin{algorithmic}[1]
\State \textbf{Input:} $keywords$, $data\_chunks$, $model$, $thresholds$
\State \textbf{Output:} $FAQ\_pairs$, $evaluation\_results$

\State \textcolor{primary}{// Phase 1: Question Generation}
\State $questions \gets []$
\For{$chunk \in data\_chunks$}
    \State $relevant\_kw \gets$ ExtractRelevantKeywords($chunk$, $keywords$)
    \For{$kw \in relevant\_kw$}
        \State $prompt \gets$ ConstructPrompt(\{
        \State \quad type: "question",
        \State \quad chunk: $chunk$,
        \State \quad keyword: $kw$,
        \State \quad constraints: \{specificity: HIGH, insightfulness: HIGH\},
        \State \quad few\_shot: $question\_examples$
        \State \})
        \State $q \gets model$.Generate($prompt$, temperature=0.7)
        \State \textcolor{secondary}{// Enforce constraints}
        \If{NOT ValidateQuestionConstraints($q$)}
            \State $q \gets$ RegenerateWithStricterPrompt($q$, $chunk$, $kw$)
        \EndIf
        \State $questions$.append(\{question: $q$, chunk: $chunk$, keyword: $kw$\})
    \EndFor
\EndFor

\State \textcolor{primary}{// Phase 2: Answer Generation}
\State $faq\_pairs \gets []$
\For{$item \in questions$}
    \State $q \gets item$.question
    \State $context \gets item$.chunk
    \State $prompt \gets$ ConstructPrompt(\{
    \State \quad type: "answer",
    \State \quad question: $q$,
    \State \quad context: $context$,
    \State \quad constraints: \{groundedness: STRICT, specificity: HIGH\},
    \State \quad few\_shot: $answer\_examples$
    \State \})
    \State $a \gets model$.Generate($prompt$, temperature=0.2)
    \State \textcolor{secondary}{// Verify groundedness}
    \State $claims \gets$ ExtractClaims($a$)
    \State $grounded \gets$ VerifyGroundedness($claims$, $context$, threshold=0.8)
    \If{$grounded <$ MIN\_GROUNDEDNESS}
        \State $a \gets$ RegenerateGrounded($q$, $context$, $claims$)
    \EndIf
    \State $faq\_pairs$.append(\{question: $q$, answer: $a$, chunk\_id: $item$.chunk.id\})
\EndFor

\State \textcolor{primary}{// Phase 3: Evaluation}
\State $results \gets []$
\For{$pair \in faq\_pairs$}
    \State \textcolor{secondary}{// Statistical graders}
    \State $scores\_stat \gets$ \{
    \State \quad coverage\_q: ComputeCoverageStatistical($pair$.question, $data\_chunks$),
    \State \quad coverage\_a: ComputeCoverageStatistical($pair$.answer, $pair$.chunk),
    \State \quad specificity\_q: ComputeSpecificityStatistical($pair$.question),
    \State \quad specificity\_a: ComputeSpecificityStatistical($pair$.answer),
    \State \quad insightfulness\_q: ComputeInsightfulnessStatistical($pair$.question),
    \State \quad insightfulness\_a: ComputeInsightfulnessStatistical($pair$.answer),
    \State \quad groundedness\_a: ComputeGroundednessStatistical($pair$.answer, $pair$.chunk)
    \State \}
    
    \State \textcolor{secondary}{// LLM graders}
    \State $scores\_llm \gets$ \{
    \State \quad coverage\_q: GradeLLM($pair$.question, "coverage", $few\_shot\_coverage$),
    \State \quad coverage\_a: GradeLLM($pair$.answer, "coverage", $few\_shot\_coverage$),
    \State \quad specificity\_q: GradeLLM($pair$.question, "specificity", $few\_shot\_spec$),
    \State \quad specificity\_a: GradeLLM($pair$.answer, "specificity", $few\_shot\_spec$),
    \State \quad insightfulness\_q: GradeLLM($pair$.question, "insightfulness", $few\_shot\_insight$),
    \State \quad insightfulness\_a: GradeLLM($pair$.answer, "insightfulness", $few\_shot\_insight$),
    \State \quad groundedness\_a: GradeLLM($pair$.answer, "groundedness", $few\_shot\_ground$)
    \State \}
    
    \State \textcolor{secondary}{// Combine scores}
    \State $final\_scores \gets$ CombineScores($scores\_stat$, $scores\_llm$, weights=\{stat: 0.5, llm: 0.5\})
    \State $results$.append(\{pair: $pair$, scores\_stat: $scores\_stat$, scores\_llm: $scores\_llm$, final: $final\_scores$\})
\EndFor

\State \textcolor{primary}{// Phase 4: Export to Excel}
\State ExportToExcel($results$, columns=[
\State \quad "question", "answer", "chunk\_id", "keyword",
\State \quad "coverage\_math\_q", "coverage\_llm\_q", "coverage\_final\_q",
\State \quad "coverage\_math\_a", "coverage\_llm\_a", "coverage\_final\_a",
\State \quad "...", "overall\_score"
\State ])

\State \textbf{Return} $faq\_pairs$, $results$
\end{algorithmic}
\end{algorithm}
</algorithm>

\section{Practical Considerations}

\subsection{Computational Efficiency}

\begin{keypoint}{Optimization Strategies}
\begin{itemize}
    \item \textbf{Batch Processing:} Generate multiple Q\&A pairs in parallel
    \item \textbf{Caching:} Store embeddings and computed similarities
    \item \textbf{Early Stopping:} Skip evaluation if generation fails quality checks
    \item \textbf{Sampling:} For large corpora, evaluate representative subset
\end{itemize}
\end{keypoint}

\textbf{Cost-Complexity Trade-off:}

\begin{equation}
\text{Cost} = n_{questions} \times (c_{gen} + c_{stat} + c_{LLM})
\end{equation}

where:
\begin{itemize}
    \item $c_{gen}$: Generation cost (2 LLM calls per Q\&A)
    \item $c_{stat}$: Statistical grading (negligible)
    \item $c_{LLM}$: LLM grading cost ($\sim$8 calls per Q\&A pair)
\end{itemize}

\subsection{Quality Thresholds}

Recommended minimum thresholds based on use case:

\begin{table}[h]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Use Case} & \textbf{Groundedness} & \textbf{Specificity} & \textbf{Insightfulness} \\
\midrule
Customer Support & $\geq 0.90$ & $\geq 0.75$ & $\geq 0.60$ \\
Technical Docs & $\geq 0.95$ & $\geq 0.80$ & $\geq 0.70$ \\
Educational Content & $\geq 0.90$ & $\geq 0.70$ & $\geq 0.80$ \\
Marketing FAQs & $\geq 0.85$ & $\geq 0.65$ & $\geq 0.75$ \\
\bottomrule
\end{tabular}
\caption{Recommended Quality Thresholds by Use Case}
\end{table}

\subsection{Iterative Refinement}

\begin{center}
\begin{tikzpicture}[
  node distance=1cm and 1.5cm,
  box/.style={rectangle, rounded corners, minimum width=2cm, minimum height=0.8cm, text width=1.8cm, text centered, draw=black, font=\scriptsize\bfseries, drop shadow}
]

\node (generate) [box, fill=primary!20] {Generate FAQs};
\node (evaluate) [box, below=of generate, fill=accent!20] {Evaluate};
\node (analyze) [box, below=of evaluate, fill=secondary!20] {Analyze in Excel};
\node (identify) [box, below=of analyze, fill=warning!20] {Identify Low Scores};
\node (adjust) [box, left=of identify, fill=success!20] {Adjust Prompts/Constraints};
\node (regenerate) [box, above=of adjust, fill=primary!20] {Regenerate Subset};

\draw [-{Stealth[length=2mm]}, thick] (generate) -- (evaluate);
\draw [-{Stealth[length=2mm]}, thick] (evaluate) -- (analyze);
\draw [-{Stealth[length=2mm]}, thick] (analyze) -- (identify);
\draw [-{Stealth[length=2mm]}, thick] (identify) -- (adjust);
\draw [-{Stealth[length=2mm]}, thick] (adjust) -- (regenerate);
\draw [-{Stealth[length=2mm]}, thick, dashed, color=accent] (regenerate) -- (generate);

\node[font=\tiny, text width=1.5cm, align=center] at (3,-3.5) {Iterate until quality targets met};

\end{tikzpicture}
\end{center}

\section{Case Study Example}

\subsection{Scenario}

\textbf{Domain:} API Documentation for a payment processing platform \\
\textbf{Keywords:} \textit{authentication, webhooks, refunds, disputes, PCI compliance} \\
\textbf{Chunks:} 247 documentation sections \\
\textbf{Target:} Generate 100 high-quality FAQ pairs

\subsection{Results Summary}

\begin{table}[h]
\centering
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Metric} & \textbf{Initial Avg} & \textbf{After Iteration 1} & \textbf{After Iteration 2} & \textbf{Target} \\
\midrule
Coverage (Q) & 0.72 & 0.84 & 0.91 & $\geq 0.85$ \\
Specificity (Q) & 0.68 & 0.79 & 0.86 & $\geq 0.80$ \\
Insightfulness (Q) & 0.61 & 0.73 & 0.82 & $\geq 0.75$ \\
Groundedness (A) & 0.88 & 0.93 & 0.96 & $\geq 0.90$ \\
Specificity (A) & 0.74 & 0.83 & 0.89 & $\geq 0.80$ \\
Insightfulness (A) & 0.66 & 0.78 & 0.84 & $\geq 0.75$ \\
\midrule
\textbf{Overall Quality} & \textbf{0.715} & \textbf{0.817} & \textbf{0.880} & $\geq 0.80$ \\
\bottomrule
\end{tabular}
\caption{Case Study: Iterative Quality Improvement}
\end{table}

\subsection{Key Improvements}

\begin{enumerate}
    \item \textbf{Iteration 1 Adjustments:}
    \begin{itemize}
        \item Added more few-shot examples emphasizing specificity
        \item Increased temperature for question generation (0.5 $\to$ 0.7) to improve diversity
        \item Implemented stricter groundedness verification with NLI model
    \end{itemize}
    
    \item \textbf{Iteration 2 Adjustments:}
    \begin{itemize}
        \item Refined prompt templates to encourage "why" and "how" questions
        \item Added constraint requiring at least one concrete example in answers
        \item Implemented post-generation enhancement step for low-scoring pairs
    \end{itemize}
\end{enumerate}

\section{Limitations and Future Work}

\subsection{Current Limitations}

\begin{itemize}
    \item \textbf{Subjectivity:} Insightfulness remains somewhat subjective despite formalization
    \item \textbf{Context Dependence:} Optimal metric weights vary by domain
    \item \textbf{Computational Cost:} LLM grading adds significant latency
    \item \textbf{Language Limitation:} Framework primarily tested on English content
\end{itemize}

\subsection{Future Directions}

\begin{keypoint}{Research Opportunities}
\begin{enumerate}
    \item \textbf{Active Learning:} Use evaluation scores to guide training of specialized FAQ generation models
    \item \textbf{User Feedback Integration:} Incorporate actual user satisfaction ratings into metric refinement
    \item \textbf{Cross-lingual Extension:} Adapt framework for multilingual FAQ generation
    \item \textbf{Dynamic Re-ranking:} Real-time FAQ re-ordering based on query patterns
    \item \textbf{Automated Prompt Optimization:} Use reinforcement learning to optimize generation prompts
\end{enumerate}
\end{keypoint}

\section{Conclusion}

We have presented a comprehensive framework for RAG-based FAQ generation and evaluation that addresses the full lifecycle from generation to quality assessment. Our key contributions include:

\begin{enumerate}
    \item \textbf{Multi-dimensional evaluation}: Four complementary metrics (Coverage, Specificity, Insightfulness, Groundedness) providing holistic quality assessment
    
    \item \textbf{Hybrid grading approach}: Combining statistical/mathematical methods with LLM-based evaluation for robustness
    
    \item \textbf{Proactive quality enforcement}: Strategies to ensure quality during generation, not just measure it post-hoc
    
    \item \textbf{Keyword-driven coverage}: Leveraging user engagement data to ensure relevant topic coverage
    
    \item \textbf{Iterative refinement}: Systematic pipeline for continuous quality improvement
\end{enumerate}

\begin{insight}{Final Recommendation}
The framework is most effective when:
\begin{itemize}
    \item Groundedness is treated as a hard constraint (never compromise)
    \item Metric weights are tuned to specific use cases
    \item Few-shot examples are domain-specific and regularly updated
    \item Quality thresholds trigger automatic regeneration
    \item Human review validates a sample of generated FAQs
\end{itemize}
\end{insight}

By systematically measuring and enforcing quality across multiple dimensions, this framework enables scalable generation of high-quality FAQs that effectively serve user information needs while maintaining factual accuracy and providing genuine insight.

\vfill

\begin{center}
\textcolor{primary}{\rule{0.8\textwidth}{0.5pt}}

\textit{This framework represents a synthesis of retrieval-augmented generation, \\
multi-objective optimization, and rigorous quality assurance—\\
enabling AI systems to not just generate content, but to generate \textbf{excellent} content.}

\textcolor{primary}{\rule{0.8\textwidth}{0.5pt}}
\end{center}

\end{document}